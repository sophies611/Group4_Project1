---
title: "Project1"
author: "Sophie Shao, Sophia Yang, Han (Andy) Xu, Ken Bai"
date: "10/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis


```{r, cache=TRUE}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]
summary(tele_train)


```



## clustering

```{r}

set.seed(123)

tele_norm_cluster <- tele_norm[, -match("yyes",names(tele_norm))]

tele_clusters <- kmeans(tele_norm_cluster, 5)

## Step 4: Evaluating model performance ----
# look at the size of the clusters
tele_clusters$size

# look at the cluster centers
tele_clusters$centers
tele_clusters$cluster

tele_norm_cluster$cluster <- tele_clusters$cluster
tele_norm_cluster$yyes <- tele_norm$yyes

tapply(tele_norm_cluster$yyes, tele_norm_cluster$cluster, mean)
aggregate(data=tele_norm_cluster, yyes~cluster, mean)

#use 4 datasets for 4 clusters, 3 models eaach dataset

#combined: if 2/3 models say 1, it will be 1


str(tele_norm_cluster$cluster)
#create 4 datasets

cluster_1 <- tele_norm_cluster[tele_norm_cluster$cluster==1,]
cluster_2 <- tele_norm_cluster[tele_norm_cluster$cluster==2,]
cluster_3 <- tele_norm_cluster[tele_norm_cluster$cluster==3,]
cluster_4 <- tele_norm_cluster[tele_norm_cluster$cluster==4,]
cluster_5 <- tele_norm_cluster[tele_norm_cluster$cluster==5,]
```


### Cluster 1

```{r}
set.seed(12345)
test_set <- sample(1:nrow(cluster_1), 10000) 
teleann_train_1 <- cluster_1[-test_set,]
teleann_test_1 <- cluster_1[test_set,]
teleann_model <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(5,3))
plot(teleann_model)

```

### Cluster 2

### Cluster 3

### Cluster 4

### Cluster 5


> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## Libraries
```{r}
library(neuralnet)
library(gmodels)
library(caret)
library(class)
threshold <- 0.6
```

## ANN Model
```{r}
teleann_train <- tele_norm[-test_set,]
teleann_test <- tele_norm[test_set,]
teleann_model <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(5,3))
plot(teleann_model)
```

## Evaluate ANN Model
```{r, cache=TRUE}
teleann_pred <- predict(teleann_model, tele_test)
teleann_pred <- ifelse(teleann_pred < threshold, 0, 1)

CrossTable(x = teleann_test$yyes, y = teleann_pred, prop.chisq=FALSE)
model1results=confusionMatrix(as.factor(teleann_pred), as.factor(teleann_test$yyes), positive = "1")
```

## Improve ANN Model
```{r, cache=TRUE}
#teleann_model <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden = 5)
```

## KNN Model & Evaluation
```{r, cache=TRUE}
teleknn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = sqrt(nrow(tele_train)))
CrossTable(x = tele_test_labels, y = teleknn_pred, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_pred), as.factor(tele_test_labels), positive = "1")
```

## Improve KNN Model
```{r}
#We want to lower the k value because the data is skewed, and we ended up with k = 70 since it yields the best performing model
teleknn_pred1 <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = 70)
CrossTable(x = tele_test_labels, y = teleknn_pred1, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_pred1), as.factor(tele_test_labels), positive = "1")
```

## Logistic Regression
```{r}
str(tele)
summary(tele)
```

## Getting Train and Test Samples for logistic regression
```{r}
#tele_log_model <- glm(y~ age +job + education, data=tele_train_log, family="binomial")
tele_train$y <- tele_train_labels
tele_test$y <- tele_test_labels
tele_log_model <- glm(y~., data = tele_train, family = "binomial")
#summary(tele_log_model)
predict_values <- predict(tele_log_model, newdata=tele_test, type="response")
modpred <- as.factor(ifelse(predict_values <= threshold, 0, 1))
CrossTable(x = tele_test_labels, y = modpred, prop.chisq=FALSE)
confusionMatrix(modpred, as.factor(tele_test_labels), positive = "1")
```
