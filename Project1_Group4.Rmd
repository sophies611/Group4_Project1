---
title: "Project1"
author: "Sophie Shao, Sophia Yang, Han (Andy) Xu, Ken Bai, Shenyi (Elaine) Ge"
date: "10/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Telemarketing Project

Objective: [Sophia]


## Downloading and Prepping the Data


```{r}
# Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

# We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call.
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis
```{r, cache=TRUE}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# We are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```

## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000)
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
# First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

# Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]
summary(tele_train)
```

## Import Libraries
```{r}
library(neuralnet)
library(gmodels)
library(caret)
library(class)
threshold <- 0.6
```

## Clustering

We are using the clustering method to identify and group similar data points. This way we can identify subsets to focus on calling. 
```{r}
set.seed(123)

tele_norm_cluster <- tele_norm[, -match("yyes",names(tele_norm))]

tele_clusters <- kmeans(tele_norm_cluster, 5)

## Evaluating model performance
# Look at the size of the clusters
tele_clusters$size

# Look at the cluster centers
tele_clusters$centers
tele_clusters$cluster

tele_norm_cluster$cluster <- tele_clusters$cluster
tele_norm_cluster$yyes <- tele_norm$yyes

tapply(tele_norm_cluster$yyes, tele_norm_cluster$cluster, mean)
aggregate(data=tele_norm_cluster, yyes~cluster, mean)

# Create 4 datasets
cluster_1 <- tele_norm_cluster[tele_norm_cluster$cluster==1,]
cluster_2 <- tele_norm_cluster[tele_norm_cluster$cluster==2,]
cluster_3 <- tele_norm_cluster[tele_norm_cluster$cluster==3,]
cluster_4 <- tele_norm_cluster[tele_norm_cluster$cluster==4,]
cluster_5 <- tele_norm_cluster[tele_norm_cluster$cluster==5,]
```

<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD


=======
>>>>>>> e12a0cd6b7e2060752cef213b95cd06d48601e6a
### ANN Model

Separate the normalized data into a training and test set to test the ANN model.
```{r, cache=TRUE}
teleann_train <- tele_norm[-test_set,]
teleann_test <- tele_norm[test_set,-match("yyes",names(tele_norm))]
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]
teleann_model <- neuralnet(formula = yyes ~ ., data = teleann_train)
plot(teleann_model)
```

### Evaluate ANN Model
```{r, cache=TRUE}
teleann_pred <- predict(teleann_model, tele_test)
teleann_pred <- ifelse(teleann_pred < threshold, 0, 1)
CrossTable(x = teleann_test$yyes, y = teleann_pred, prop.chisq=FALSE)
model1results=confusionMatrix(as.factor(teleann_pred), as.factor(teleann_test$yyes), positive = "1")
```

### Improve ANN Model
```{r, cache=TRUE}
#teleann_model_improved <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden = c(5,3))

#teleann_model_1 <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(2), threshold=0.1)

#teleann_model_1a <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(2,2), threshold=0.1, stepmax=10^6)

#teleann_model_1b <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(1), threshold=0.1, stepmax=10^6, learningrate=0.01)

#teleann_model_1c <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(5), threshold=0.01)

#teleann_model_1d <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(5,2), threshold=0.05)

#teleann_model_1e <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(1,2), threshold=0.1, stepmax=10^10, learningrate=0.01)


#teleann_model_1g <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(10,5), threshold=0.1, stepmax=10^10, learningrate=0.001)

#teleann_model_1h <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(5,3), threshold=0.01)
```

### Final ANN Model
```{r, cache=TRUE}
teleann_model <- neuralnet(formula = yyes ~ ., data = teleann_train, hidden=c(2), threshold=0.01, stepmax=10^10, learningrate=0.001)
```

## KNN Model & Evaluation
```{r, cache=TRUE}
teleknn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = sqrt(nrow(tele_train)))
CrossTable(x = tele_test_labels, y = teleknn_pred, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_pred), as.factor(tele_test_labels), positive = "1")
```

## Improve KNN Model
```{r}
# We want to lower the k value because the data is skewed, and we ended up with k = 70 since it yields the best performing model
teleknn_pred1 <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = 70)
CrossTable(x = tele_test_labels, y = teleknn_pred1, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_pred1), as.factor(tele_test_labels), positive = "1")
<<<<<<< HEAD
```
=======
<<<<<<< HEAD

>>>>>>> 7e3c205cbb0da085b8ad901fe555342b8221fec2

## Logistic Regression
```{r}
str(tele)
summary(tele)
=======
>>>>>>> e12a0cd6b7e2060752cef213b95cd06d48601e6a
```

## Getting Train and Test Samples for logistic regression
```{r}
#tele_log_model <- glm(y~ age +job + education, data=tele_train_log, family="binomial")
tele_train$y <- tele_train_labels
tele_test$y <- tele_test_labels
tele_log_model <- glm(y~., data = tele_train, family = "binomial")
#summary(tele_log_model)
predict_values <- predict(tele_log_model, newdata=tele_test, type="response")
modpred <- as.factor(ifelse(predict_values <= threshold, 0, 1))
CrossTable(x = tele_test_labels, y = modpred, prop.chisq=FALSE)
confusionMatrix(modpred, as.factor(tele_test_labels), positive = "1")
```
>>>>>>> d24a0819cddce710fb4b032cfa7bd2d6770c5bfe

<<<<<<< HEAD
=======
>>>>>>> c9c316760ef760c5c88e98d5a24a97bad475b2f4
### Cluster 1

Since Cluster 1 yielded the highest call acceptance rate, we will run our models on the other 4 clusters. We then separate each cluster's data into a training and testing set, and then run each model on the training set and subsequently evaluate it on the test data. 

For the KNN...

For the ANN, we used a learning rate of 0.01, which is the amount that the weights are updated during training. Setting the learning rate very low was very computationally expensive and caused our model to run very slowly. However, we also did not want to set it too high as this would cause the model to converge too quickly. Additionally, we set the threshold to be 0.05, an increase from the default 0.01 so that the model would run faster. This means that the error only needs to change by 0.05 until the model stops optimizing. Also, we set a boundary for the stepmax so that it stops converging after a set amount of iterations so our model does not fail to converge.

For the Logistic Regression, 

For both the ANN and Logistic Regression, 

### Cluster 2
```{r, cache=TRUE}
set.seed(12345)
test_set2 <- sample(1:nrow(cluster_2), 2500)
tele_train_labels_2 <- cluster_2[-test_set2, "yyes"]
tele_test_labels_2 <- cluster_2[test_set2, "yyes"]
tele_train_2 <- cluster_2[-test_set2,]
tele_test_2 <- cluster_2[test_set2,]

#KNN
#teleknn_model2 <- knn(train = tele_train_2, test = tele_test_2, cl = tele_train_labels2, k = sqrt(nrow(tele_train_2)))
teleknn_model2_improved <- knn(train = tele_train_2, test = tele_test_2, cl = tele_train_labels_2, k = 5)
CrossTable(x = tele_test_labels_2, y = teleknn_model2_improved, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_model2_improved), as.factor(tele_test_labels_2), positive = "1")

#ANN

tele_ann_train_2 <- cluster_2[-test_set2, ]
tele_ann_train_2$yyes <- ifelse(tele_ann_train_2$yyes > 0, .9, .1) #only for ANN & logistic
tele_ann_test_2 <- cluster_2[test_set2,-match("yyes",names(cluster_2))]

teleann_model_2 <- neuralnet(formula = yyes ~ ., data = tele_ann_train_2, hidden=c(2), threshold=0.05, stepmax=10^10, learningrate=0.01)
plot(teleann_model_2)
model_results <- compute(teleann_model_2, tele_ann_test_2)
predicted_strength <- model_results$net.result
model_results <- ifelse(predicted_strength>=threshold, 1,0) #convert thresholds
cor(predicted_strength, tele_test_labels_2)
CrossTable(x = tele_test_labels_2, y = model_results, prop.chisq = FALSE)
confusionMatrix(as.factor(model_results), as.factor(tele_test_labels_2), positive = "1")


#Logistic
tele_train_2$y <- tele_train_labels_2
tele_test_2$y <- tele_test_labels_2

tele_log_model_2 <- glm(y~.+ maritalmarried* housingyes + educationuniversity.degree*maritalmarried, data = tele_ann_train_2, family = "binomial")
predict_values_2 <- predict(tele_log_model_2, newdata=tele_ann_test_2, type="response")
modpred_2 <- as.factor(ifelse(predict_values_2 <= threshold, 0, 1))
CrossTable(x = tele_test_labels_2, y = modpred_2, prop.chisq=FALSE)
confusionMatrix(modpred_2, as.factor(tele_test_labels_2), positive = "1")


cluster_2_vote <- data.frame(teleknn_model2_improved, model_results, modpred_2)
```

### Cluster 3
```{r, cache=TRUE}
set.seed(12345)
test_set3 <- sample(1:nrow(cluster_3), 1500)
tele_train_labels_3 <- cluster_3[-test_set3, "yyes"]
tele_test_labels_3 <- cluster_3[test_set3, "yyes"]
tele_train_3 <- cluster_3[-test_set3,]
tele_test_3 <- cluster_3[test_set3,]

#KNN
#teleknn_model3 <- knn(train = tele_train_3, test = tele_test_3, cl = tele_train_labels_3, k = sqrt(nrow(tele_train_3)))
teleknn_model3_improved <- knn(train = tele_train_3, test = tele_test_3, cl = tele_train_labels_3, k = 5)
CrossTable(x = tele_test_labels_3, y = teleknn_model3_improved, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_model3_improved), as.factor(tele_test_labels3), positive = "1")

#ANN

tele_ann_train_3 <- cluster_3[-test_set3, ]
tele_ann_train_3$yyes <- ifelse(tele_ann_train_3$yyes > 0, .9, .1) #only for ANN & logistic
teleann_test_3 <- cluster_3[test_set3,-match("yyes",names(cluster_3))]

teleann_model_3 <- neuralnet(formula = yyes ~ ., data = tele_ann_train_3, hidden=c(2), threshold=0.05, stepmax=10^10, learningrate=0.01)
plot(teleann_model_3)
model_results <- compute(teleann_model_3, teleann_test_3)
predicted_strength <- model_results$net.result
model_results <- ifelse(predicted_strength>=threshold, 1,0) #convert thresholds
cor(predicted_strength, tele_test_labels_3)
CrossTable(x = tele_test_labels_3, y = model_results, prop.chisq = FALSE)
confusionMatrix(as.factor(model_results), as.factor(tele_test_labels_3), positive = "1")


#Logistic
tele_train_3$y <- tele_train_labels_3
tele_test_3$y <- tele_test_labels_3

tele_log_model_3 <- glm(y~.+ maritalmarried* housingyes + educationuniversity.degree*maritalmarried, data = tele_ann_train_3, family = "binomial")
predict_values_3 <- predict(tele_log_model_3, newdata=teleann_test_3, type="response")
modpred_3 <- as.factor(ifelse(predict_values_3 <= threshold, 0, 1))
CrossTable(x = tele_test_labels_3, y = modpred_3, prop.chisq=FALSE)
confusionMatrix(modpred_3, as.factor(tele_test_labels_3), positive = "1")
```

### Cluster 4
```{r}
set.seed(12345)
test_set4 <- sample(1:nrow(cluster_4), 1500)
tele_train_labels_4 <- cluster_4[-test_set4, "yyes"]
tele_test_labels_4 <- cluster_4[test_set4, "yyes"]
tele_train_4 <- cluster_4[-test_set4,]
tele_test_4 <- cluster_4[test_set4,]

#KNN
#teleknn_model4 <- knn(train = tele_train_4, test = tele_test_4, cl = tele_train_labels_4, k = sqrt(nrow(tele_train_4)))
teleknn_model4_improved <- knn(train = tele_train_4, test = tele_test_4, cl = tele_train_labels_4, k = 1)
CrossTable(x = tele_test_labels_4, y = teleknn_model4_improved, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_model4_improved), as.factor(tele_test_labels_4), positive = "1")


#ANN

tele_ann_train_4 <- cluster_4[-test_set4, ]
tele_ann_train_4$yyes <- ifelse(tele_ann_train_4$yyes > 0, .9, .1) #only for ANN & logistic
tele_ann_test_4 <- cluster_4[test_set4,-match("yyes",names(cluster_4))]

teleann_model_4 <- neuralnet(formula = yyes ~ ., data = tele_ann_train_4, hidden=c(2), threshold=0.05, stepmax=10^10, learningrate=0.01)
plot(teleann_model_4)
model_results <- compute(teleann_model_4, tele_ann_test_4)
predicted_strength <- model_results$net.result
model_results <- ifelse(predicted_strength>=threshold, 1,0) #convert thresholds
cor(predicted_strength, tele_test_labels_4)
CrossTable(x = tele_test_labels_4, y = model_results, prop.chisq = FALSE)
confusionMatrix(as.factor(model_results), as.factor(tele_test_labels_4), positive = "1")
#Logistic
tele_train_4$y <- tele_train_labels_4
tele_test_4$y <- tele_test_labels_4

tele_log_model_4 <- glm(y~.+ maritalmarried* housingyes + educationuniversity.degree*maritalmarried, data = tele_ann_train_4, family = "binomial")
predict_values_4 <- predict(tele_log_model_4, newdata=tele_ann_test_4, type="response")
modpred_4 <- as.factor(ifelse(predict_values_4 <= threshold, 0, 1))
CrossTable(x = tele_test_labels_4, y = modpred_4, prop.chisq=FALSE)
confusionMatrix(modpred_4, as.factor(tele_test_labels_4), positive = "1")
```

### Cluster 5
```{r}
set.seed(12345)
test_set5 <- sample(1:nrow(cluster_5), 2000)
tele_train_labels_5 <- cluster_5[-test_set5, "yyes"]
tele_test_labels_5 <- cluster_5[test_set5, "yyes"]
tele_train_5 <- cluster_5[-test_set5,]
tele_test_5 <- cluster_5[test_set5,]

#KNN
#teleknn_model5 <- knn(train = tele_train_5, test = tele_test_5, cl = tele_train_labels5, k = sqrt(nrow(tele_train_5)))
teleknn_model5_improved <- knn(train = tele_train_5, test = tele_test_5, cl = tele_train_labels_5, k = 1)
CrossTable(x = tele_test_labels_5, y = teleknn_model5_improved, prop.chisq = FALSE)
confusionMatrix(as.factor(teleknn_model5_improved), as.factor(tele_test_labels_5), positive = "1")


#ANN


tele_ann_train_5 <- cluster_5[-test_set5, ]
tele_ann_train_5$yyes <- ifelse(tele_ann_train_5$yyes > 0, .9, .1) #only for ANN & logistic
tele_ann_test_5 <- cluster_5[test_set5,-match("yyes",names(cluster_5))]

teleann_model_5 <- neuralnet(formula = yyes ~ ., data = tele_ann_train_5, hidden=c(2), threshold=0.05, stepmax=10^10, learningrate=0.01)
plot(teleann_model_5)
model_results <- compute(teleann_model_5, tele_ann_test_5)
predicted_strength <- model_results$net.result
model_results <- ifelse(predicted_strength>=threshold, 1,0) #convert thresholds
cor(predicted_strength, tele_test_labels_5)
CrossTable(x = tele_test_labels_5, y = model_results, prop.chisq = FALSE)
confusionMatrix(as.factor(model_results), as.factor(tele_test_labels_5), positive = "1")

#Logistic
tele_train_5$y <- tele_train_labels_5
tele_test_5$y <- tele_test_labels_5

tele_log_model_5 <- glm(y~.+ maritalmarried* housingyes + educationuniversity.degree*maritalmarried, data = tele_ann_train_5, family = "binomial")
predict_values_5 <- predict(tele_log_model_5, newdata=tele_ann_test_5, type="response")
modpred_5 <- as.factor(ifelse(predict_values_5 <= threshold, 0, 1))
CrossTable(x = tele_test_labels_5, y = modpred_5, prop.chisq=FALSE)
confusionMatrix(modpred_5, as.factor(tele_test_labels_5), positive = "1")
```


#Voting Model
```{r}





```


